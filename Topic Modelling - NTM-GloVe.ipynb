{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f804ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your DataFrame with text data\n",
    "df = pd.read_csv('sectionized_data.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to each document in df['Body']\n",
    "df['Body'] = df['Body'].astype(str)\n",
    "processed_docs = df['Body'].apply(preprocess_text)\n",
    "\n",
    "# Create TaggedDocument for training the Doc2Vec model\n",
    "tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(processed_docs)]\n",
    "\n",
    "# Train Doc2Vec model\n",
    "doc2vec_model = gensim.models.Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=20)\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "# Extract document embeddings\n",
    "document_embeddings = [doc2vec_model.docvecs[str(i)] for i in range(len(tagged_data))]\n",
    "\n",
    "# Convert document embeddings to GloVe-like format\n",
    "document_vectors = np.array(document_embeddings)\n",
    "\n",
    "# Define the NTM-GloVe model using TensorFlow\n",
    "class NTM_GloVe_TensorFlow:\n",
    "    def __init__(self, num_topics):\n",
    "        self.num_topics = num_topics\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(512, activation='relu', input_shape=(100,)),\n",
    "            Dense(self.num_topics, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Function to compute coherence score for a given number of topics\n",
    "def compute_coherence_score(num_topics, document_vectors):\n",
    "    # Convert document embeddings to one-hot encodings (for topics)\n",
    "    document_vectors_onehot = np.eye(num_topics)[np.random.choice(num_topics, document_vectors.shape[0])]\n",
    "    \n",
    "    # Train NTM-GloVe model\n",
    "    ntm_glove_tf_model = NTM_GloVe_TensorFlow(num_topics)\n",
    "    ntm_glove_tf_model.fit(document_vectors, document_vectors_onehot)\n",
    "    \n",
    "    # Get the topic distributions for each document\n",
    "    document_topic_distributions = ntm_glove_tf_model.transform(document_vectors)\n",
    "    \n",
    "    # Convert document_topic_distributions to topic vectors\n",
    "    topic_vectors = np.argmax(document_topic_distributions, axis=1)\n",
    "    \n",
    "    # Convert topic vectors to topic distribution matrix\n",
    "    topic_distribution_matrix = np.eye(num_topics)[topic_vectors]\n",
    "    \n",
    "    # Convert topic_distribution_matrix to a list of topic lists\n",
    "    topic_lists = topic_distribution_matrix.tolist()\n",
    "    \n",
    "    # Create a dictionary for the coherence model\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    \n",
    "    # Compute coherence score\n",
    "    coherence_model = CoherenceModel(topics=topic_lists, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    \n",
    "    return coherence_score\n",
    "\n",
    "# Range of possible numbers of topics to explore\n",
    "min_topics = 2\n",
    "max_topics = 100\n",
    "\n",
    "# List to store coherence scores\n",
    "coherence_scores = []\n",
    "\n",
    "# Iterate over the range of topic numbers and compute coherence scores\n",
    "for num_topics in range(min_topics, max_topics + 1):\n",
    "    coherence_score = compute_coherence_score(num_topics, document_vectors)\n",
    "    coherence_scores.append(coherence_score)\n",
    "\n",
    "# Find the optimal number of topics based on the maximum coherence score\n",
    "optimal_num_topics = min_topics + coherence_scores.index(max(coherence_scores))\n",
    "\n",
    "print(\"Optimal number of topics:\", optimal_num_topics)\n",
    "\n",
    "# Convert document embeddings to one-hot encodings (for topics)\n",
    "document_vectors_onehot = np.eye(optimal_num_topics)[np.random.choice(optimal_num_topics, document_vectors.shape[0])]\n",
    "\n",
    "# Train NTM-GloVe model with optimal number of topics\n",
    "ntm_glove_tf_model = NTM_GloVe_TensorFlow(optimal_num_topics)\n",
    "ntm_glove_tf_model.fit(document_vectors, document_vectors_onehot)\n",
    "\n",
    "# Get the topic distributions for each document\n",
    "document_topic_distributions = ntm_glove_tf_model.transform(document_vectors)\n",
    "\n",
    "# Print topic distributions for all documents\n",
    "for i, doc_topic_distribution in enumerate(document_topic_distributions):\n",
    "    print(f\"Topic Distribution for Document {i + 1}:\")\n",
    "    print(doc_topic_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot topic distributions for all documents\n",
    "for i, doc_topic_distribution in enumerate(document_topic_distributions):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(range(len(doc_topic_distribution)), doc_topic_distribution, color='skyblue')\n",
    "    plt.xlabel('Topic')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'Topic Distribution for Document {i + 1}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da7129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate coherence score\n",
    "def calculate_coherence_score(model, documents, dictionary, coherence='c_v'):\n",
    "    # Get the topics\n",
    "    topics = model.model.get_weights()[0]  # Assuming the topic weights are in the first layer\n",
    "    \n",
    "    # Convert topic weights to token IDs\n",
    "    topic_token_ids = []\n",
    "    for topic_weights in topics:\n",
    "        topic_token_ids.append([token_id for token_id, weight in sorted(dictionary.token2id.items(), key=lambda x: x[1])])\n",
    "\n",
    "    # Compute coherence score\n",
    "    coherence_model = CoherenceModel(topics=topic_token_ids, texts=documents, dictionary=dictionary, coherence=coherence)\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    \n",
    "    return coherence_score\n",
    "\n",
    "# Create a Gensim Dictionary\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Calculate coherence score\n",
    "coherence_score = calculate_coherence_score(ntm_glove_tf_model, processed_docs, dictionary)\n",
    "print(\"Coherence Score:\", coherence_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca23566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store dominant topics\n",
    "dominant_topics = []\n",
    "\n",
    "# Iterate through the document topic distributions\n",
    "for doc_topic_distribution in document_topic_distributions:\n",
    "    # Find the index of the topic with the highest probability\n",
    "    dominant_topic_index = np.argmax(doc_topic_distribution)\n",
    "    # Append the dominant topic index to the list\n",
    "    dominant_topics.append(dominant_topic_index)\n",
    "\n",
    "# Print the dominant topics for each document\n",
    "for i, dominant_topic in enumerate(dominant_topics):\n",
    "    print(f\"Document {i + 1}: Dominant Topic {dominant_topic}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
