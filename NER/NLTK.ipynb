{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7674adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your .jsonl file\n",
    "file_path = '/Users/kowsalya/Downloads/admin.jsonl'\n",
    "\n",
    "data_list = []\n",
    "\n",
    "# Read the first line of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:  # This iterates over each line until the end of the file\n",
    "        line = line.strip()\n",
    "        if line:  # Check if line is not empty\n",
    "            line_data = json.loads(line) # Parse the JSON content\n",
    "            data_list.append(line_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0621fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Function to tokenize essay into sentences with start and end char offsets\n",
    "def sentence_tokenize_with_offsets(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    offsets = []\n",
    "    start = 0\n",
    "    for sentence in sentences:\n",
    "        start = text.find(sentence, start)\n",
    "        end = start + len(sentence)\n",
    "        offsets.append((start, end))\n",
    "        start = end\n",
    "    return sentences, offsets\n",
    "\n",
    "# Function to tokenize sentences into words with their global char offsets\n",
    "def word_tokenize_with_global_offsets(sentences, sentence_offsets, text):\n",
    "    global_word_offsets = []\n",
    "    for sentence, (sent_start, sent_end) in zip(sentences, sentence_offsets):\n",
    "        words = word_tokenize(sentence)\n",
    "        start = sent_start\n",
    "        for word in words:\n",
    "            # Find the start of the word within the essay\n",
    "            start = text.find(word, start)\n",
    "            end = start + len(word)\n",
    "            global_word_offsets.append((start, end))\n",
    "            start = end\n",
    "    return global_word_offsets\n",
    "\n",
    "# Apply labels based on character offsets to tokens\n",
    "def apply_labels(tokens_offsets, labels):\n",
    "    tagged_tokens = []\n",
    "    for start, end in tokens_offsets:\n",
    "        label = \"O\"  # Default label\n",
    "        for l_start, l_end, l_label in labels:\n",
    "            if start >= l_start and end <= l_end:\n",
    "                if start == l_start:\n",
    "                    label = \"B-\" + l_label\n",
    "                else:\n",
    "                    label = \"I-\" + l_label\n",
    "                break\n",
    "        tagged_tokens.append(label)\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6a0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(prepared_data, index):\n",
    "    \"\"\"\n",
    "    Extract features for a token at the given index in the prepared data.\n",
    "    \n",
    "    :param prepared_data: List of tuples [(token, pos, iob), ...]\n",
    "    :param index: Index of the current token in the prepared data\n",
    "    :return: Dictionary of features\n",
    "    \"\"\"\n",
    "    # Current token, POS tag, and IOB tag\n",
    "    token, pos_tag, _ = prepared_data[index]\n",
    "    \n",
    "    # Features from the current token\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': token.lower(),\n",
    "        'word.isupper()': token.isupper(),\n",
    "        'word.istitle()': token.istitle(),\n",
    "        'word.isdigit()': token.isdigit(),\n",
    "        'pos': pos_tag,  # Current POS tag\n",
    "    }\n",
    "    \n",
    "    # Features from the previous token\n",
    "    if index > 0:\n",
    "        prev_token, prev_pos_tag, _ = prepared_data[index - 1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': prev_token.lower(),\n",
    "            '-1:word.isupper()': prev_token.isupper(),\n",
    "            '-1:word.istitle()': prev_token.istitle(),\n",
    "            '-1:pos': prev_pos_tag,\n",
    "        })\n",
    "    else:\n",
    "        # Indicate that it's the start of a sentence/document\n",
    "        features['BOS'] = True\n",
    "\n",
    "    # Features from the next token\n",
    "    if index < len(prepared_data) - 1:\n",
    "        next_token, next_pos_tag, _ = prepared_data[index + 1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': next_token.lower(),\n",
    "            '+1:word.isupper()': next_token.isupper(),\n",
    "            '+1:word.istitle()': next_token.istitle(),\n",
    "            '+1:pos': next_pos_tag,\n",
    "        })\n",
    "    else:\n",
    "        # Indicate that it's the end of a sentence/document\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc9f3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prepared_data = []  # This will hold the prepared_data for all documents\n",
    "\n",
    "for data in data_list:\n",
    "    # Process the essay\n",
    "    sentences, sentence_offsets = sentence_tokenize_with_offsets(data['text'])\n",
    "    tokens_offsets = word_tokenize_with_global_offsets(sentences, sentence_offsets, data['text'])\n",
    "    labels = apply_labels(tokens_offsets, data['label'])\n",
    "\n",
    "    # Assuming you now want to combine tokens, POS tags, and IOB tags\n",
    "    tokens = [data['text'][start:end] for start, end in tokens_offsets]\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    prepared_data = [(token, pos, label) for ((token, pos), label) in zip(pos_tags, labels)]\n",
    "    all_prepared_data.append(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9890860",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_featuresets = []  # This will hold the featuresets for all documents\n",
    "\n",
    "for prepared_data in all_prepared_data:\n",
    "    featuresets = []\n",
    "    for i in range(len(prepared_data)):\n",
    "        features = extract_features(prepared_data, i)\n",
    "        label = prepared_data[i][2]  # The IOB tag is the third element in the tuple\n",
    "        featuresets.append((features, label))\n",
    "    all_featuresets.append(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a0e9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Shuffle the data to ensure random distribution\n",
    "random.shuffle(all_featuresets)\n",
    "\n",
    "# Calculate split indices\n",
    "total_documents = len(all_featuresets)\n",
    "train_end = int(total_documents * 0.6)\n",
    "validation_end = train_end + int(total_documents * 0.2)\n",
    "\n",
    "# Split the data\n",
    "train_data = all_featuresets[:train_end]\n",
    "validation_data = all_featuresets[train_end:validation_end]\n",
    "test_data = all_featuresets[validation_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "# Flatten the training data if it's a list of lists\n",
    "train_data_flat = [item for sublist in train_data for item in sublist]\n",
    "\n",
    "# Train the model\n",
    "classifier = MaxentClassifier.train(train_data_flat, 'IIS', trace=0, max_iter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4041f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4088644609303081\n",
      "Test Accuracy: 0.417397962055959\n"
     ]
    }
   ],
   "source": [
    "# Flatten the validation and test data if they're lists of lists\n",
    "validation_data_flat = [item for sublist in validation_data for item in sublist]\n",
    "test_data_flat = [item for sublist in test_data for item in sublist]\n",
    "\n",
    "# Evaluate on the validation set\n",
    "validation_accuracy = nltk.classify.accuracy(classifier, validation_data_flat)\n",
    "print(f\"Validation Accuracy: {validation_accuracy}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy = nltk.classify.accuracy(classifier, test_data_flat)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "import nltk\n",
    "\n",
    "# Assuming train_set and validation_set are already defined\n",
    "performance_records = {}\n",
    "\n",
    "# Range of `max_iter` values to try\n",
    "max_iter_options = [10, 50, 100, 200]\n",
    "\n",
    "for max_iter in max_iter_options:\n",
    "    print(f\"Training model with max_iter={max_iter}\")\n",
    "    classifier = MaxentClassifier.train(train_set, 'IIS', trace=0, max_iter=max_iter)\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    validation_accuracy = nltk.classify.accuracy(classifier, validation_set)\n",
    "    print(f\"Validation Accuracy for max_iter={max_iter}: {validation_accuracy}\")\n",
    "    \n",
    "    # Record the performance\n",
    "    performance_records[max_iter] = validation_accuracy\n",
    "\n",
    "# Identify the best `max_iter` based on validation performance\n",
    "best_max_iter = max(performance_records, key=performance_records.get)\n",
    "best_accuracy = performance_records[best_max_iter]\n",
    "print(f\"Best max_iter: {best_max_iter} with Validation Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Retrain the model with the best `max_iter`\n",
    "best_classifier = MaxentClassifier.train(train_set, 'IIS', trace=0, max_iter=best_max_iter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
