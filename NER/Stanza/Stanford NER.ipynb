{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc977eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(91632) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef44808b38a46929a83f808d0fe1eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 20:32:58 INFO: Downloaded file to /Users/kowsalya/stanza_resources/resources.json\n",
      "2024-04-10 20:32:58 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-04-10 20:32:59 INFO: File exists: /Users/kowsalya/stanza_resources/en/default.zip\n",
      "2024-04-10 20:33:05 INFO: Finished downloading models and saved to /Users/kowsalya/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download the English model with NER\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0c90ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your .jsonl file\n",
    "file_path = '/Users/kowsalya/Downloads/admin.jsonl'\n",
    "\n",
    "data_list = []\n",
    "\n",
    "# Read the first line of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:  # This iterates over each line until the end of the file\n",
    "        line = line.strip()\n",
    "        if line:  # Check if line is not empty\n",
    "            line_data = json.loads(line) # Parse the JSON content\n",
    "            data_list.append(line_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adedca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Randomly shuffle the list\n",
    "random.shuffle(data_list)\n",
    "\n",
    "# Calculate split indices\n",
    "first_split = int(len(data_list) * 0.6)\n",
    "second_split = first_split + int(len(data_list) * 0.2)\n",
    "\n",
    "# Split the list\n",
    "train = data_list[:first_split]\n",
    "dev = data_list[first_split:second_split]\n",
    "test = data_list[second_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "492fd611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_label_spans(text, labels):\n",
    "    \"\"\"\n",
    "    Adjusts label spans to ensure they do not start or end with whitespace.\n",
    "    \"\"\"\n",
    "    adjusted_labels = []\n",
    "    for start, end, label_type in labels:\n",
    "        # Adjust start index if it points to a space\n",
    "        while text[start] == ' ' and start < end:\n",
    "            start += 1\n",
    "        # Adjust end index if it points to a space\n",
    "        while text[end] == ' ' and end > start:\n",
    "            end -= 1\n",
    "        adjusted_labels.append((start, end, label_type))\n",
    "    return adjusted_labels\n",
    "\n",
    "def convert_to_bioes_adjusted(text, labels):\n",
    "    tokens = word_tokenize(text)\n",
    "    token_offsets = []\n",
    "    cursor = 0\n",
    "\n",
    "    # Tokenize and record the character offsets of each token\n",
    "    for token in tokens:\n",
    "        start = text.find(token, cursor)\n",
    "        end = start + len(token) - 1  # Adjust for inclusive range\n",
    "        token_offsets.append((start, end))\n",
    "        cursor = end + 1\n",
    "\n",
    "    # Initialize BIOES tags as 'O'\n",
    "    bioes_tags = ['O'] * len(tokens)\n",
    "\n",
    "    # Adjust labels to ensure they do not start or end on whitespace\n",
    "    adjusted_labels = adjust_label_spans(text, labels)\n",
    "\n",
    "    # Apply BIOES tagging\n",
    "    for start, end, label_type in adjusted_labels:\n",
    "        start_token = end_token = None\n",
    "        for i, (t_start, t_end) in enumerate(token_offsets):\n",
    "            if start >= t_start and start <= t_end:\n",
    "                start_token = i\n",
    "            if end >= t_start and end <= t_end:\n",
    "                end_token = i\n",
    "                break\n",
    "        if start_token is not None and end_token is not None:\n",
    "            if start_token == end_token:  # Single-token entity\n",
    "                bioes_tags[start_token] = f\"S-{label_type}\"\n",
    "            else:\n",
    "                bioes_tags[start_token] = f\"B-{label_type}\"\n",
    "                for j in range(start_token + 1, end_token):\n",
    "                    bioes_tags[j] = f\"I-{label_type}\"\n",
    "                bioes_tags[end_token] = f\"E-{label_type}\"\n",
    "\n",
    "    return list(zip(tokens, bioes_tags))\n",
    "\n",
    "\n",
    "def save_to_bioes_file(filename, tokens_tags):\n",
    "    \"\"\"\n",
    "    Saves the tokens and their BIOES tags to a file in BIOES format.\n",
    "    \n",
    "    :param filename: Path to the output file.\n",
    "    :param tokens_tags: A list of tuples, each containing a token and its BIOES tag.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    with open(filename, 'a', encoding='utf-8') as f:\n",
    "        previous_tag = None\n",
    "        for token, tag in tokens_tags:\n",
    "            # Write token and tag separated by a space\n",
    "            f.write(f\"{token} {tag}\\n\")\n",
    "            \n",
    "            # Add a blank line if the current tag is the end of an entity or if the previous tag was 'O' and the current tag starts an entity\n",
    "            if tag.startswith('E-') or tag.startswith('S-') or (previous_tag == 'O' and tag.startswith('B-')):\n",
    "                f.write(\"\\n\")\n",
    "            previous_tag = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0200711",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in train:\n",
    "    output_filename = \"data/ner/en_sample.train.bioes\"\n",
    "    tokens_bioes_adjusted = convert_to_bioes_adjusted(doc[\"text\"], doc[\"label\"])\n",
    "    save_to_bioes_file(output_filename, tokens_bioes_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2071c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in dev:\n",
    "    output_filename = \"data/ner/en_sample.dev.bioes\"\n",
    "    tokens_bioes_adjusted = convert_to_bioes_adjusted(doc[\"text\"], doc[\"label\"])\n",
    "    save_to_bioes_file(output_filename, tokens_bioes_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e22e6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test:\n",
    "    output_filename = \"data/ner/en_sample.test.bioes\"\n",
    "    tokens_bioes_adjusted = convert_to_bioes_adjusted(doc[\"text\"], doc[\"label\"])\n",
    "    save_to_bioes_file(output_filename, tokens_bioes_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1261ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2975 examples loaded from data/ner/en_sample.train.bioes\n",
      "Generated json file data/ner/en_sample.train.json\n",
      "1074 examples loaded from data/ner/en_sample.dev.bioes\n",
      "Generated json file data/ner/en_sample.dev.json\n",
      "1335 examples loaded from data/ner/en_sample.test.bioes\n",
      "Generated json file data/ner/en_sample.test.json\n"
     ]
    }
   ],
   "source": [
    "import stanza.utils.datasets.ner.prepare_ner_file as prepare_ner_file\n",
    "\n",
    "output_jsons = ['data/ner/en_sample.train.json', 'data/ner/en_sample.dev.json', 'data/ner/en_sample.test.json']\n",
    "input_bioes = ['data/ner/en_sample.train.bioes', 'data/ner/en_sample.dev.bioes', 'data/ner/en_sample.test.bioes']\n",
    "for i in range(3):\n",
    "    prepare_ner_file.process_dataset(input_bioes[i], output_jsons[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "122016de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60477) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-10 20:01:09 INFO: Training program called with:\n",
      "/Users/kowsalya/anaconda3/lib/python3.11/site-packages/stanza/utils/training/run_ner.py en_sample --max_steps 500 --word_emb_dim 5\n",
      "2024-04-10 20:01:09 DEBUG: en_sample: en_sample\n",
      "2024-04-10 20:01:09 INFO: Using model /Users/kowsalya/stanza_resources/en/forward_charlm/1billion.pt for forward charlm\n",
      "2024-04-10 20:01:09 INFO: Using model /Users/kowsalya/stanza_resources/en/backward_charlm/1billion.pt for backward charlm\n",
      "2024-04-10 20:01:09 INFO: Using default pretrain for en:sample, found in /Users/kowsalya/stanza_resources/en/pretrain/fasttextcrawl.pt  To use a different pretrain, specify --wordvec_pretrain_file\n",
      "2024-04-10 20:01:09 INFO: en_sample: saved_models/ner/en_sample_charlm_nertagger.pt does not exist, training new model\n",
      "2024-04-10 20:01:09 INFO: Using model /Users/kowsalya/stanza_resources/en/forward_charlm/1billion.pt for forward charlm\n",
      "2024-04-10 20:01:09 INFO: Using model /Users/kowsalya/stanza_resources/en/backward_charlm/1billion.pt for backward charlm\n",
      "2024-04-10 20:01:09 INFO: Using default pretrain for en:sample, found in /Users/kowsalya/stanza_resources/en/pretrain/fasttextcrawl.pt  To use a different pretrain, specify --wordvec_pretrain_file\n",
      "2024-04-10 20:01:09 INFO: Running train step with args: ['--train_file', 'data/ner/en_sample.train.json', '--eval_file', 'data/ner/en_sample.dev.json', '--shorthand', 'en_sample', '--mode', 'train', '--charlm', '--charlm_shorthand', 'en_1billion', '--charlm_forward_file', '/Users/kowsalya/stanza_resources/en/forward_charlm/1billion.pt', '--charlm_backward_file', '/Users/kowsalya/stanza_resources/en/backward_charlm/1billion.pt', '--wordvec_pretrain_file', '/Users/kowsalya/stanza_resources/en/pretrain/fasttextcrawl.pt', '--max_steps', '500', '--word_emb_dim', '5']\n",
      "2024-04-10 20:01:09 INFO: Running NER tagger in train mode\n",
      "2024-04-10 20:01:09 INFO: Directory saved_models/ner does not exist; creating...\n",
      "2024-04-10 20:01:09 INFO: ARGS USED AT TRAINING TIME:\n",
      "batch_size: 32\n",
      "bert_finetune: False\n",
      "bert_hidden_layers: None\n",
      "bert_learning_rate: 1.0\n",
      "bert_model: None\n",
      "char: True\n",
      "char_dropout: 0\n",
      "char_emb_dim: 100\n",
      "char_hidden_dim: 100\n",
      "char_lowercase: False\n",
      "char_num_layers: 1\n",
      "char_rec_dropout: 0\n",
      "charlm: True\n",
      "charlm_backward_file: /Users/kowsalya/stanza_resources/en/backward_charlm/1billion.pt\n",
      "charlm_forward_file: /Users/kowsalya/stanza_resources/en/forward_charlm/1billion.pt\n",
      "charlm_save_dir: saved_models/charlm\n",
      "charlm_shorthand: en_1billion\n",
      "connect_output_layers: False\n",
      "data_dir: data/ner\n",
      "device: cpu\n",
      "dropout: 0.5\n",
      "emb_finetune: True\n",
      "emb_finetune_known_only: False\n",
      "eval_file: data/ner/en_sample.dev.json\n",
      "eval_interval: 500\n",
      "eval_output_file: None\n",
      "finetune: False\n",
      "finetune_load_name: None\n",
      "gradient_checkpointing: False\n",
      "hidden_dim: 256\n",
      "ignore_tag_scores: None\n",
      "input_transform: True\n",
      "locked_dropout: 0.0\n",
      "log_norms: False\n",
      "log_step: 20\n",
      "lora_alpha: 128\n",
      "lora_dropout: 0.1\n",
      "lora_modules_to_save: []\n",
      "lora_rank: 64\n",
      "lora_target_modules: ['query', 'value', 'output.dense', 'intermediate.dense']\n",
      "lowercase: True\n",
      "lr: 0.1\n",
      "lr_decay: 0.5\n",
      "max_grad_norm: 5.0\n",
      "max_steps: 500\n",
      "max_steps_no_improve: 2500\n",
      "min_lr: 0.0001\n",
      "mode: train\n",
      "momentum: 0\n",
      "num_layers: 1\n",
      "optim: sgd\n",
      "patience: 3\n",
      "predict_tagset: None\n",
      "pretrain_max_vocab: 100000\n",
      "rec_dropout: 0\n",
      "sample_train: 1.0\n",
      "save_dir: saved_models/ner\n",
      "save_name: en_sample_charlm_nertagger.pt\n",
      "scheme: bioes\n",
      "second_bert_learning_rate: 0\n",
      "second_lr: 0.005\n",
      "second_optim: None\n",
      "seed: 1234\n",
      "shorthand: en_sample\n",
      "train_classifier_only: False\n",
      "train_file: data/ner/en_sample.train.json\n",
      "train_scheme: None\n",
      "use_peft: False\n",
      "wandb: False\n",
      "wandb_name: None\n",
      "word_dropout: 0.01\n",
      "word_emb_dim: 5\n",
      "wordvec_dir: extern_data/word2vec\n",
      "wordvec_file: \n",
      "wordvec_pretrain_file: /Users/kowsalya/stanza_resources/en/pretrain/fasttextcrawl.pt\n",
      "\n",
      "2024-04-10 20:01:09 DEBUG: Loaded pretrain from /Users/kowsalya/stanza_resources/en/pretrain/fasttextcrawl.pt\n",
      "2024-04-10 20:01:09 WARNING: Embedding file has a dimension of 300.  Model will be built with that size instead of 5\n",
      "2024-04-10 20:01:09 INFO: Using pretrained contextualized char embedding\n",
      "2024-04-10 20:01:09 INFO: Loading training data with batch size 32 from data/ner/en_sample.train.json\n",
      "2024-04-10 20:01:10 INFO: Loaded 2975 sentences of training data\n",
      "2024-04-10 20:01:10 DEBUG: Creating delta vocab of size 4923\n",
      "2024-04-10 20:01:10 DEBUG: 93 batches created.\n",
      "2024-04-10 20:01:10 INFO: Loading dev data from data/ner/en_sample.dev.json\n",
      "2024-04-10 20:01:10 INFO: Loaded 1074 sentences of dev data\n",
      "2024-04-10 20:01:10 DEBUG: 34 batches created.\n",
      "2024-04-10 20:01:10 INFO: Training data has 1 columns of tags\n",
      "2024-04-10 20:01:10 INFO: Tags present in training set at column 0:\n",
      "  Tags without BIES markers: O\n",
      "  Tags with B-, I-, E-, or S-: CASE_NUMBER COURT DATE GPE JUDGE LAWYER ORG OTHER_PERSON PETITIONER PRECEDENT PROVISION RESPONDENT STATUTE WITNESS\n",
      "2024-04-10 20:01:10 INFO: Training tagger...\n",
      "2024-04-10 20:01:11 DEBUG: Building SGD with lr=0.100000, momentum=0.000000\n",
      "2024-04-10 20:01:11 WARNING: Found tags in dev set missing from the expected tag set: ['I-WITNESS']\n",
      "2024-04-10 20:01:11 INFO: NERTagger(\n",
      "  (word_emb): Embedding(100000, 300, padding_idx=0)\n",
      "  (delta_emb): Embedding(4923, 300, padding_idx=0)\n",
      "  (charmodel_forward): CharacterLanguageModel(\n",
      "    (char_emb): Embedding(948, 100)\n",
      "    (charlstm): PackedLSTM(\n",
      "      (lstm): LSTM(100, 1024, batch_first=True)\n",
      "    )\n",
      "    (decoder): Linear(in_features=1024, out_features=948, bias=True)\n",
      "    (dropout): Dropout(p=0.05, inplace=False)\n",
      "    (char_dropout): SequenceUnitDropout(p=1e-05, replacement_id=1)\n",
      "  )\n",
      "  (charmodel_backward): CharacterLanguageModel(\n",
      "    (char_emb): Embedding(948, 100)\n",
      "    (charlstm): PackedLSTM(\n",
      "      (lstm): LSTM(100, 1024, batch_first=True)\n",
      "    )\n",
      "    (decoder): Linear(in_features=1024, out_features=948, bias=True)\n",
      "    (dropout): Dropout(p=0.05, inplace=False)\n",
      "    (char_dropout): SequenceUnitDropout(p=1e-05, replacement_id=1)\n",
      "  )\n",
      "  (input_transform): Linear(in_features=2348, out_features=2348, bias=True)\n",
      "  (taggerlstm): PackedLSTM(\n",
      "    (lstm): LSTM(2348, 256, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (tag_clfs): ModuleList(\n",
      "    (0): Linear(in_features=512, out_features=56, bias=True)\n",
      "  )\n",
      "  (crits): ModuleList(\n",
      "    (0): CRFLoss()\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (worddrop): WordDropout(p=0.01)\n",
      "  (lockeddrop): LockedDropout(p=0.0)\n",
      ")\n",
      "/Users/kowsalya/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "2024-04-10 20:02:24 INFO: 2024-04-10 20:02:24: step 20/500, loss = 11.530809 (4.600 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:03:03 INFO: 2024-04-10 20:03:03: step 40/500, loss = 6.494950 (1.917 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:04:34 INFO: 2024-04-10 20:04:34: step 60/500, loss = 5.285378 (1.612 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:05:20 INFO: 2024-04-10 20:05:20: step 80/500, loss = 5.581998 (2.660 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:06:14 INFO: 2024-04-10 20:06:14: step 100/500, loss = 7.244215 (2.240 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:07:09 INFO: 2024-04-10 20:07:09: step 120/500, loss = 8.027280 (3.671 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:08:17 INFO: 2024-04-10 20:08:17: step 140/500, loss = 5.753729 (1.010 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:09:10 INFO: 2024-04-10 20:09:10: step 160/500, loss = 9.338737 (2.094 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:10:33 INFO: 2024-04-10 20:10:33: step 180/500, loss = 3.997804 (8.588 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:11:34 INFO: 2024-04-10 20:11:34: step 200/500, loss = 7.774246 (4.231 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:13:03 INFO: 2024-04-10 20:13:03: step 220/500, loss = 2.794931 (2.525 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:14:04 INFO: 2024-04-10 20:14:04: step 240/500, loss = 8.058097 (3.584 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:14:49 INFO: 2024-04-10 20:14:49: step 260/500, loss = 3.265657 (1.536 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:15:33 INFO: 2024-04-10 20:15:33: step 280/500, loss = 4.562062 (1.046 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:16:34 INFO: 2024-04-10 20:16:34: step 300/500, loss = 2.645901 (3.001 sec/batch), lr: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-10 20:17:17 INFO: 2024-04-10 20:17:17: step 320/500, loss = 2.683011 (1.951 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:18:22 INFO: 2024-04-10 20:18:22: step 340/500, loss = 2.047757 (8.031 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:19:35 INFO: 2024-04-10 20:19:35: step 360/500, loss = 3.501266 (1.940 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:20:36 INFO: 2024-04-10 20:20:36: step 380/500, loss = 2.389971 (1.549 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:22:02 INFO: 2024-04-10 20:22:02: step 400/500, loss = 2.475004 (2.972 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:23:21 INFO: 2024-04-10 20:23:21: step 420/500, loss = 2.207893 (1.146 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:24:09 INFO: 2024-04-10 20:24:09: step 440/500, loss = 2.058061 (2.294 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:25:17 INFO: 2024-04-10 20:25:17: step 460/500, loss = 3.483303 (4.824 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:26:36 INFO: 2024-04-10 20:26:36: step 480/500, loss = 2.783307 (10.910 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:27:57 INFO: 2024-04-10 20:27:57: step 500/500, loss = 18.206520 (3.992 sec/batch), lr: 0.100000\n",
      "2024-04-10 20:27:57 INFO: Evaluating on dev set...\n",
      "2024-04-10 20:29:03 INFO: Score by entity:\n",
      "Prec.\tRec.\tF1\n",
      "26.05\t27.16\t26.59\n",
      "2024-04-10 20:29:03 INFO: step 500: train_loss = 6.512209, dev_score = 0.2659\n",
      "2024-04-10 20:29:04 INFO: Model saved to saved_models/ner/en_sample_charlm_nertagger.pt\n",
      "2024-04-10 20:29:04 INFO: New best model saved.\n",
      "2024-04-10 20:29:04 INFO: \n",
      "2024-04-10 20:29:04 INFO: stopping...\n",
      "2024-04-10 20:29:04 INFO: Training ended with 500 steps.\n",
      "2024-04-10 20:29:04 INFO: Best dev F1 = 26.59, at iteration = 500\n",
      "2024-04-10 20:29:04 INFO: Running dev step with args: ['--eval_file', 'data/ner/en_sample.dev.json', '--shorthand', 'en_sample', '--mode', 'predict', '--charlm', '--charlm_shorthand', 'en_1billion', '--charlm_forward_file', '/Users/kowsalya/stanza_resources/en/forward_charlm/1billion.pt', '--charlm_backward_file', '/Users/kowsalya/stanza_resources/en/backward_charlm/1billion.pt', '--wordvec_pretrain_file', '/Users/kowsalya/stanza_resources/en/pretrain/fasttextcrawl.pt', '--max_steps', '500', '--word_emb_dim', '5']\n",
      "2024-04-10 20:29:04 INFO: Running NER tagger in predict mode\n",
      "2024-04-10 20:29:04 DEBUG: Loaded pretrain from /Users/kowsalya/stanza_resources/en/pretrain/fasttextcrawl.pt\n",
      "2024-04-10 20:29:04 DEBUG: Building SGD with lr=0.100000, momentum=0.000000\n",
      "2024-04-10 20:29:04 DEBUG: Loaded model for eval from saved_models/ner/en_sample_charlm_nertagger.pt\n",
      "2024-04-10 20:29:04 DEBUG: Using the 0 tagset for evaluation\n",
      "2024-04-10 20:29:04 INFO: Loading data with batch size 32...\n",
      "2024-04-10 20:29:05 DEBUG: 34 batches created.\n",
      "2024-04-10 20:29:05 WARNING: Found tags in eval_file missing from the expected tag set: ['I-WITNESS']\n",
      "2024-04-10 20:29:05 INFO: Start evaluation...\n",
      "2024-04-10 20:30:07 INFO: Score by entity:\n",
      "Prec.\tRec.\tF1\n",
      "26.05\t27.16\t26.59\n",
      "2024-04-10 20:30:07 INFO: Score by token:\n",
      "Prec.\tRec.\tF1\n",
      "40.09\t38.70\t39.38\n",
      "2024-04-10 20:30:07 INFO: Weighted f1 for non-O tokens: 0.398450\n",
      "2024-04-10 20:30:07 INFO: NER tagger score: en_sample saved_models/ner/en_sample_charlm_nertagger.pt data/ner/en_sample.dev.json 26.59\n",
      "2024-04-10 20:30:07 INFO: NER Entity F1 scores:\n",
      "  CASE_NUMBER: 0.00\n",
      "  COURT: 52.17\n",
      "  DATE: 96.09\n",
      "  GPE: 66.67\n",
      "  JUDGE: 4.17\n",
      "  LAWYER: 13.84\n",
      "  ORG: 19.21\n",
      "  OTHER_PERSON: 13.48\n",
      "  PETITIONER: 11.63\n",
      "  PRECEDENT: 52.50\n",
      "  PROVISION: 82.35\n",
      "  RESPONDENT: 0.00\n",
      "  STATUTE: 51.61\n",
      "  WITNESS: 0.00\n",
      "2024-04-10 20:30:07 INFO: NER token confusion matrix:\n",
      "          t\\p                   O    CASE_NUMBER          COURT           DATE            GPE          JUDGE         LAWYER            ORG   OTHER_PERSON     PETITIONER      PRECEDENT      PROVISION     RESPONDENT        STATUTE        WITNESS\n",
      "                 O          19890              0              0              2              0              0              0              1              0             21              0              1              0              0              0\n",
      "       CASE_NUMBER              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0\n",
      "             COURT              0              0             14              0              0              0              0              0              0              0              2              0              0              0              0\n",
      "              DATE              2              0              0            209              0              0              0              0              0              0              0              0              0              0              0\n",
      "               GPE              0              0              0              0              2              0              0              1              0              0              0              0              0              0              0\n",
      "             JUDGE              4              2              2              0              0              3              9              8              9             14             13              0              0              0              0\n",
      "            LAWYER              0              0              0              0              0              0             27             17             33             34              8              0              0              0              0\n",
      "               ORG             13              0              0              0              0              0              6             87              0              2              2              0              0              4              0\n",
      "      OTHER_PERSON              0              0              0              0              0              0             29             18              6              6              1              0              0              0              0\n",
      "        PETITIONER              0              0              1              0              1              0              3            125              0             21              0              0              0              0              0\n",
      "         PRECEDENT              3              0              0              1              0              0              2             35              3             19            475              2              0              2              0\n",
      "         PROVISION              1              0              0              1              0              0              0              0              0              0              0             47              0              0              0\n",
      "        RESPONDENT             65              0              0              0              0              0              0            214              0              2              0              0              0              0              0\n",
      "           STATUTE              2              0              0              0              0              0              0              6              0              0              0              0              0             21              0\n",
      "           WITNESS              0              4              0              0              0              0             30             76             10            115              6              1              0              0              0\n",
      "2024-04-10 20:30:07 INFO: Running test step with args: ['--eval_file', 'data/ner/en_sample.test.json', '--shorthand', 'en_sample', '--mode', 'predict', '--charlm', '--charlm_shorthand', 'en_1billion', '--charlm_forward_file', '/Users/kowsalya/stanza_resources/en/forward_charlm/1billion.pt', '--charlm_backward_file', '/Users/kowsalya/stanza_resources/en/backward_charlm/1billion.pt', '--wordvec_pretrain_file', '/Users/kowsalya/stanza_resources/en/pretrain/fasttextcrawl.pt', '--max_steps', '500', '--word_emb_dim', '5']\n",
      "2024-04-10 20:30:07 INFO: Running NER tagger in predict mode\n",
      "2024-04-10 20:30:07 DEBUG: Loaded pretrain from /Users/kowsalya/stanza_resources/en/pretrain/fasttextcrawl.pt\n",
      "2024-04-10 20:30:07 DEBUG: Building SGD with lr=0.100000, momentum=0.000000\n",
      "2024-04-10 20:30:07 DEBUG: Loaded model for eval from saved_models/ner/en_sample_charlm_nertagger.pt\n",
      "2024-04-10 20:30:07 DEBUG: Using the 0 tagset for evaluation\n",
      "2024-04-10 20:30:07 INFO: Loading data with batch size 32...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-10 20:30:08 DEBUG: 42 batches created.\n",
      "2024-04-10 20:30:08 INFO: Start evaluation...\n",
      "2024-04-10 20:31:09 INFO: Score by entity:\n",
      "Prec.\tRec.\tF1\n",
      "58.04\t60.82\t59.40\n",
      "2024-04-10 20:31:09 INFO: Score by token:\n",
      "Prec.\tRec.\tF1\n",
      "40.77\t40.09\t40.43\n",
      "2024-04-10 20:31:09 INFO: Weighted f1 for non-O tokens: 0.391684\n",
      "2024-04-10 20:31:09 INFO: NER tagger score: en_sample saved_models/ner/en_sample_charlm_nertagger.pt data/ner/en_sample.test.json 59.40\n",
      "2024-04-10 20:31:09 INFO: NER Entity F1 scores:\n",
      "  CASE_NUMBER: 77.27\n",
      "  COURT: 66.67\n",
      "  DATE: 97.66\n",
      "  GPE: 52.94\n",
      "  JUDGE: 23.33\n",
      "  LAWYER: 30.53\n",
      "  ORG: 75.14\n",
      "  OTHER_PERSON: 24.12\n",
      "  PETITIONER: 0.00\n",
      "  PRECEDENT: 27.69\n",
      "  PROVISION: 73.91\n",
      "  RESPONDENT: 0.00\n",
      "  STATUTE: 36.84\n",
      "  WITNESS: 0.00\n",
      "2024-04-10 20:31:09 INFO: NER token confusion matrix:\n",
      "          t\\p                   O    CASE_NUMBER          COURT           DATE            GPE          JUDGE         LAWYER            ORG   OTHER_PERSON     PETITIONER      PRECEDENT      PROVISION     RESPONDENT        STATUTE        WITNESS\n",
      "                 O          21271              0              0              0              0              0              1              0              0              0              1              1              0              0              0\n",
      "       CASE_NUMBER              1             44              0              2              0              0              0              1              0              0              2              7              0              3              0\n",
      "             COURT              0              0             44              0              1              0              0              1              0              0              0              0              0              6              0\n",
      "              DATE              0              0              0            333              0              0              0              1              0              0              0              1              0              0              0\n",
      "               GPE              0              0              0              0              9              0              1              4              2              0              1              0              0              0              0\n",
      "             JUDGE              3              0              3              0              0              9             12              5              6             10             12              0              0              0              0\n",
      "            LAWYER              0              0              1              0              0              0             46              6             31              8              0              0              0              0              0\n",
      "               ORG             13              1              2              3              6             11             12            509              3             26             30              0              0              7              0\n",
      "      OTHER_PERSON              2              0              1              0              0              0             36             47             24             35             14              2              0              0              0\n",
      "        PETITIONER              0              0              0              0              0              0              0            128              0              0              2              0              0              9              0\n",
      "         PRECEDENT             15              0              4              0              0              0             14             13              4             19            161              1              0              7              0\n",
      "         PROVISION              1              0              0              0              0              0              0              0              0              0              0             34              0              0              0\n",
      "        RESPONDENT              0              0              0              0              0              0             41             14              7             13              8              0              0              0              0\n",
      "           STATUTE              0              0              0              0              1              0              0              9              0              0              0              0              0             25              0\n",
      "           WITNESS              0              0              0              0              0              0              1              1              1              0              0              0              0              0              0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m stanza.utils.training.run_ner en_sample --max_steps 500 --word_emb_dim 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e6c6ffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 20:58:39 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8bc608794a4a818e75a212cd18df52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 20:58:39 INFO: Downloaded file to /Users/kowsalya/stanza_resources/resources.json\n",
      "2024-04-10 20:58:39 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-04-10 20:58:40 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-04-10 20:58:40 INFO: Using device: cpu\n",
      "2024-04-10 20:58:40 INFO: Loading: tokenize\n",
      "2024-04-10 20:58:41 INFO: Loading: mwt\n",
      "2024-04-10 20:58:41 INFO: Loading: ner\n",
      "2024-04-10 20:58:42 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford University ORG\n",
      "California GPE\n"
     ]
    }
   ],
   "source": [
    "# Loading the trained NER model\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "\n",
    "# Processing a new text\n",
    "doc = nlp(\"Stanford University is located in California.\")\n",
    "for ent in doc.entities:\n",
    "    print(ent.text, ent.type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44043495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
