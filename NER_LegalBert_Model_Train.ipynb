{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a755d32-392d-465d-ac11-c7f9ad95b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip jsonl_files.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9929ea-aca8-4f62-8c08-3da3228612ba",
   "metadata": {},
   "source": [
    "# From here on out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd455590-c431-482b-b0d0-688236deb549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/IS450_NER/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./legal_bert_ner_model5/tokenizer_config.json',\n",
       " './legal_bert_ner_model5/special_tokens_map.json',\n",
       " './legal_bert_ner_model5/vocab.txt',\n",
       " './legal_bert_ner_model5/added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model attempt 1\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "folder_path = os.path.join(\"content/jsonl_files\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "# |      >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "## |      >>> assistant_model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "# |      >>> # set pad_token_id to eos_token_id because GPT2 does not have a PAD token\n",
    "# |      >>> model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "# |      >>> input_prompt = \"It might be possible to\"\n",
    "# |      >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "# |      >>> # instantiate logits processors\n",
    "# |      >>> logits_processor = LogitsProcessorList(\n",
    "# |      ...     [\n",
    "# |      ...         MinLengthLogitsProcessor(10, eos_token_id=model.generation_config.eos_token_id),\n",
    "# |      ...     ]\n",
    "# |      ... )\n",
    "# |      >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
    "# |      >>> outputs = model.assisted_decoding(\n",
    " #|      ...     input_ids,\n",
    " #|      ...     assistant_model=assistant_model,\n",
    " #|      ...     logits_processor=logits_processor,\n",
    " #|      ...     stopping_criteria=stopping_criteria,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the label list based on your data\n",
    "label_list = [\"PRECEDENT\", \"LAWYER\", \"JUDGE\", \"RESPONDENT\", \"GPE\",\n",
    "              \"DATE\", \"OTHER_PERSON\", \"PROVISION\", \"ORG\", \"PETITIONER\",\n",
    "              \"WITNESS\", \"COURT\", \"STATUTE\", \"CASE_NUMBER\"]\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, tokenizer, label_list, max_length=512, split_ratio=0.8, seed=42):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_list = label_list\n",
    "        self.label_map = {label: i for i, label in enumerate(label_list)}\n",
    "        self.samples = []\n",
    "        self.max_length = max_length\n",
    "        self.split_ratio = split_ratio\n",
    "        self.seed = seed\n",
    "\n",
    "        self._load_data(folder_path)\n",
    "        self._split_data()\n",
    "\n",
    "    def _load_data(self, folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.jsonl'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    for line in file:\n",
    "                        data = json.loads(line)\n",
    "                        text = data['text']\n",
    "                        labels = data['label']\n",
    "                        tokenized_inputs = self.tokenizer(\n",
    "                            text,\n",
    "                            is_split_into_words=True,\n",
    "                            padding='max_length',\n",
    "                            truncation=True,\n",
    "                            max_length=self.max_length,\n",
    "                            return_tensors='pt'\n",
    "                        )\n",
    "                        labels = self.align_labels(labels, tokenized_inputs.input_ids)\n",
    "                        self.samples.append({\n",
    "                            'input_ids': tokenized_inputs.input_ids,\n",
    "                            'attention_mask': tokenized_inputs.attention_mask,\n",
    "                            'labels': labels\n",
    "                        })\n",
    "\n",
    "    def _split_data(self):\n",
    "        split_index = int(len(self.samples) * self.split_ratio)\n",
    "        self.train_samples = self.samples[:split_index]\n",
    "        self.eval_samples = self.samples[split_index:]\n",
    "\n",
    "    def align_labels(self, labels, input_ids):\n",
    "        aligned_labels = []\n",
    "        label_idx = 0\n",
    "        for i in range(input_ids.size(1)):\n",
    "            token = self.tokenizer.convert_ids_to_tokens(input_ids[0, i].item())\n",
    "            if token.startswith(\"##\"):\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                if label_idx < len(labels) and i >= labels[label_idx][0] and i <= labels[label_idx][1]:\n",
    "                    aligned_labels.append(self.label_map.get(labels[label_idx][2], -100))\n",
    "                else:\n",
    "                    aligned_labels.append(-100)\n",
    "                if label_idx < len(labels) and i == labels[label_idx][1]:\n",
    "                    label_idx += 1\n",
    "        return torch.tensor(aligned_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.train_samples[idx]\n",
    "        return sample\n",
    "\n",
    "    def get_eval_dataset(self):\n",
    "        return self.eval_samples\n",
    "\n",
    "\n",
    "class CustomDataCollatorForTokenClassification(DataCollatorForTokenClassification):\n",
    "    def __call__(self, features):\n",
    "        batch = {}\n",
    "        batch[\"input_ids\"] = torch.stack([feature[\"input_ids\"].squeeze() for feature in features])\n",
    "        batch[\"attention_mask\"] = torch.stack([feature[\"attention_mask\"].squeeze() for feature in features])\n",
    "        batch[\"labels\"] = torch.stack([feature[\"labels\"].squeeze() for feature in features])\n",
    "\n",
    "        return batch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "model = BertForTokenClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", num_labels=len(label_list))\n",
    "\n",
    "data_collator = CustomDataCollatorForTokenClassification(tokenizer)\n",
    "# Initialize dataset\n",
    "dataset = CustomDataset(folder_path, tokenizer, label_list)\n",
    "\n",
    "# Get evaluation dataset\n",
    "eval_dataset = dataset.get_eval_dataset()\n",
    "\n",
    "# Prepare training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize trainer after defining the model and label_list\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "\n",
    "\n",
    "# Define label list\n",
    "label_list = [\"PRECEDENT\", \"LAWYER\", \"JUDGE\", \"RESPONDENT\", \"GPE\",\n",
    "              \"DATE\", \"OTHER_PERSON\", \"PROVISION\", \"ORG\", \"PETITIONER\",\n",
    "              \"WITNESS\", \"COURT\", \"STATUTE\", \"CASE_NUMBER\"]\n",
    "\n",
    "# Set label mapping\n",
    "model.config.id2label = {i: label for i, label in enumerate(label_list)}\n",
    "model.config.label2id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./legal_bert_ner_model5')\n",
    "tokenizer.save_pretrained('./legal_bert_ner_model5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d494260-4015-4f74-8dc5-71db41623a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.098388671875\n",
      "Precision: 0.098388671875\n",
      "Recall: 0.098388671875\n",
      "F1 Score: 0.098388671875\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Get the predicted labels\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "\n",
    "# Extract predicted label IDs\n",
    "predicted_label_ids = np.argmax(predictions.predictions, axis=2)\n",
    "\n",
    "# Flatten the predictions and labels to calculate accuracy and F1 score\n",
    "flat_predictions = np.concatenate(predicted_label_ids)\n",
    "flat_labels = np.concatenate([eval_dataset[i][\"labels\"].numpy() for i in range(len(eval_dataset))])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(flat_labels, flat_predictions)\n",
    "\n",
    "# Calculate precision, recall, F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(flat_labels, flat_predictions, average='micro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cffd7e4d-0892-4ea2-882d-4c240739ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 11 11 ...  1  3 11]\n",
      " [ 1 11 11 ...  3  1 11]\n",
      " [11 11 11 ...  1  3 11]\n",
      " ...\n",
      " [ 1 11  3 ...  3  3 11]\n",
      " [ 1 11  3 ...  1  1 11]\n",
      " [ 1 11 11 ...  1  1 11]]\n",
      "Predicted Labels: ['RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'LAWYER', 'RESPONDENT', 'PETITIONER', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'RESPONDENT', 'LAWYER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'PETITIONER', 'COURT', 'COURT', 'PETITIONER', 'RESPONDENT', 'COURT', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'PETITIONER', 'RESPONDENT', 'PETITIONER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'PETITIONER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'COURT', 'PETITIONER', 'PETITIONER', 'COURT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'PETITIONER', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'COURT', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'PETITIONER', 'RESPONDENT', 'RESPONDENT', 'COURT', 'PETITIONER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'PETITIONER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'COURT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'COURT']\n",
      "Predicted Labels: ['LAWYER', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'LAWYER', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'PETITIONER', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'LAWYER', 'PETITIONER', 'LAWYER', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'PETITIONER', 'COURT', 'LAWYER', 'RESPONDENT', 'COURT', 'PETITIONER', 'COURT', 'LAWYER', 'PETITIONER', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'LAWYER', 'LAWYER', 'COURT', 'RESPONDENT', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'PETITIONER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'PETITIONER', 'LAWYER', 'PETITIONER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'COURT']\n",
      "Predicted Labels: ['COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'PETITIONER', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'LAWYER', 'COURT', 'PETITIONER', 'LAWYER', 'PETITIONER', 'PETITIONER', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'LAWYER', 'RESPONDENT', 'PETITIONER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'PETITIONER', 'PETITIONER', 'COURT', 'COURT', 'PETITIONER', 'COURT', 'COURT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'COURT', 'COURT', 'COURT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'COURT', 'PETITIONER', 'RESPONDENT', 'COURT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'COURT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'RESPONDENT', 'COURT', 'COURT', 'PETITIONER', 'PETITIONER', 'COURT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'COURT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'COURT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'COURT', 'COURT', 'COURT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'COURT', 'PETITIONER', 'PETITIONER', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'COURT', 'PETITIONER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'PETITIONER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'COURT']\n",
      "Predicted Labels: ['COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'PETITIONER', 'PETITIONER', 'COURT', 'COURT', 'PETITIONER', 'LAWYER', 'PETITIONER', 'PETITIONER', 'COURT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'COURT', 'LAWYER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'PETITIONER', 'COURT', 'COURT', 'COURT', 'LAWYER', 'LAWYER', 'COURT', 'COURT', 'LAWYER', 'LAWYER', 'COURT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'COURT', 'COURT', 'LAWYER', 'RESPONDENT', 'LAWYER', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'COURT', 'LAWYER', 'COURT', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'COURT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'LAWYER', 'PETITIONER', 'PETITIONER', 'LAWYER', 'RESPONDENT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'PETITIONER', 'RESPONDENT', 'LAWYER', 'PETITIONER', 'LAWYER', 'PETITIONER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'PETITIONER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'PETITIONER', 'RESPONDENT', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'PETITIONER', 'PETITIONER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'COURT']\n",
      "Predicted Labels: ['LAWYER', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'PETITIONER', 'COURT', 'COURT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'COURT', 'COURT', 'PETITIONER', 'PETITIONER', 'PETITIONER', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'PETITIONER', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'LAWYER', 'COURT', 'LAWYER', 'COURT', 'COURT', 'COURT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'COURT', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'LAWYER', 'COURT', 'LAWYER', 'COURT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'COURT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'COURT', 'RESPONDENT', 'RESPONDENT', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'LAWYER', 'RESPONDENT', 'LAWYER', 'RESPONDENT', 'LAWYER', 'COURT']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Put the model on the device\n",
    "model.to(device)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Iterate over the evaluation dataset and make predictions\n",
    "predictions = []\n",
    "for batch in eval_dataset:\n",
    "    # Move inputs to the appropriate device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, get logits\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Append predictions to the list\n",
    "    predictions.append(predicted_labels.detach().cpu().numpy())\n",
    "\n",
    "# Convert the list of predictions to a single numpy array\n",
    "predictions = np.concatenate(predictions)\n",
    "\n",
    "# Now you have predictions for all samples in the evaluation dataset\n",
    "print(predictions)\n",
    "\n",
    "# Define a function to convert integer labels to label names\n",
    "def convert_labels_to_names(predictions, label_list):\n",
    "    label_names = []\n",
    "    for pred in predictions:\n",
    "        label_names.append([label_list[idx] for idx in pred])\n",
    "    return label_names\n",
    "\n",
    "# Convert predicted labels to label names\n",
    "predicted_label_names = convert_labels_to_names(predictions, label_list)\n",
    "\n",
    "# Print some examples of predicted labels\n",
    "for i in range(5):  # Print the first 5 examples\n",
    "    print(\"Predicted Labels:\", predicted_label_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696a33a1-dc5d-4567-ab7a-116c7c1cc062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: {'input_ids': tensor([[  101, 11965,   177,  3849,  4675,  7025,   179,   166,  4473,   184,\n",
      "          6582,   185,  3378,   185,   126,   638,   128,   163,  6441,   173,\n",
      "          1147,   256,   394,   119, 12968,  5014,   118,   611,   115,  6457,\n",
      "          8322,  2311,   118,   611,   273,   254,   119,   556,   692,   638,\n",
      "          2309,   118,   240,   119,  1026,   240,  5288,  4107,   119,  7028,\n",
      "          4383,  4300, 24691,  9259,   177,   154,   173,   661,   505,   111,\n",
      "           163,   112,   119, 10648,   395,   178,   163,  7979,  3847, 20625,\n",
      "           212,  8436, 20112,  8764, 10177,  2474,   111,  6247,  3272, 15726,\n",
      "          8764, 10177,  2474,   212,  1523,   112,   217,   207,   272,   120,\n",
      "         15786, 11872,  5647,   111,  2087,   212,   408,   486,   110,   163,\n",
      "           648,   112,   217,   207,   408,   486,   276,   119, 11965,   177,\n",
      "          3849,  4675,  7025,   179,   116,   116,  4473,   184,  6582,   185,\n",
      "          3378,   185,   966,   492,   116,   409,   210,   240,   116,   331,\n",
      "          3468,   242,   159,  1123,   162,   508,   111,   199,   112,   111,\n",
      "           146,   112,   409,   210,   240,  3044,   116,   331,   163,   422,\n",
      "           111,   200,   112,  2390,  1665,   111,   393,   116,   257,  2124,\n",
      "           212,   748,   112,   325,  1029,   221,  8020,  1701,   454,   116,\n",
      "           248,   201,   111,   200,   112,   460,   325,   111,  2952,  1191,\n",
      "           115,  1094,  3884,   149,   174,   112,   115,   163,   422,   111,\n",
      "           200,   112,  2390,  1665,   111,   393,   116,   257,  2124,   212,\n",
      "           748,   112,   325,   111,  2952,  4343,   115,   722,  3884,   149,\n",
      "           174,   112,   115,   159,  1123,   162,   508,   111,   199,   112,\n",
      "           111,   146,   112,   409,   210,   240,   111,  2952,  5536,   115,\n",
      "           162,   202,   115,   611,  3884,   149,   174,   112,   384,  4866,\n",
      "           116,  2112,   210,   418,   116,  1259,  8016,   316,   232,   116,\n",
      "          2934,  2753,   256,   116,   232,   317,  7087,   212,  2487,   217,\n",
      "           291,   210,   418,   116,   331,   232,   305,   219,  3232,   218,\n",
      "           240,   116,  1557,  2841,   115,  2623,   384,  4866,   325,   111,\n",
      "          2952,  3898,   115,   689,  3884,   149,   174,   112,   556,   692,\n",
      "           638,  7028,  4383,  4300, 24691,  9259,   177,   154,   173,   119,\n",
      "          2500,   198,  2934,  2753,   189,   244,   237, 11845,   482,   210,\n",
      "           111,  2872,   112,  5367,  1111,   117,  6688,  5797,   223,   230,\n",
      "          1887,   117,   335,  3127,   227,   307,   992,  4382,   212,   237,\n",
      "         18920,   189,   178,   310,   384,  1540,   221,   705,   117,   649,\n",
      "           115,   606,   966,   212,   797,   115,  3275,  2326,   373,   207,\n",
      "           986,   117,   226,   223, 11073,   175,   115,  2144,   115,   369,\n",
      "           117,   330,   223,   145,  1082,   195,   408,  2530,  1325,   117,\n",
      "           226,   408,  2530,   223,  1528,   530,   372,   233,   223,  1287,\n",
      "           211,  5482,  6112,   189,   242,   115,   217,  1363,   115,   207,\n",
      "          1746,  2934,   325,   111,  2952,  6026,   115,   611,  3884,   149,\n",
      "           174,   112,   215,   728,   207,  6724,   473,   111,  2952,  4692,\n",
      "           115,  1658,  3884,   149,   174,   112,   117,   233,   223,   323,\n",
      "           530,   213,   966,   470,   115,  1130,   207,  1699,   839,   210,\n",
      "           266,   111,  5767,   225,   374,   211, 11628,  2208,   112,  3684,\n",
      "           115,   222,   207,  2535,   210,  2762,   115,   750,   470,   212,\n",
      "           115,  4296,   115,   750,   277,   117,   226,   223,   466,   211,\n",
      "           207, 10357,   817,   210,  2934,  2753,   189,  2149,   117,   231,\n",
      "          1540,   115,  9310,   548,   317,   212,  6346,   207,   276,   115,\n",
      "          2140,   221,   145,   519,   117,   289,   228,  1052,   223,   207,\n",
      "           251,   210,  3988,   110,   686,   115,  7812,   115,   221,   532,\n",
      "           224,   321,   115,   225,   145,   105,   901, 12218,   105,   117,\n",
      "           226,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100,    9, -100,    9, -100,    9,    9, -100,    9, -100,\n",
      "            9, -100,    9,    9,    9,    9, -100, -100,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/IS450_NER/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.7395979166030884, 'eval_runtime': 0.1644, 'eval_samples_per_second': 12.168, 'eval_steps_per_second': 6.084, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import BertForTokenClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset2(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, label_list, max_length=512, split_ratio=0.8, seed=42):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_list = label_list\n",
    "        self.label_map = {label: i for i, label in enumerate(label_list)}\n",
    "        self.samples = []\n",
    "        self.max_length = max_length\n",
    "        self.split_ratio = split_ratio\n",
    "        self.seed = seed\n",
    "\n",
    "        self._load_data(file_path)\n",
    "        self._split_data()\n",
    "\n",
    "    def _load_data(self, file_path2):\n",
    "        with open(file_path2, 'r') as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                text = data['text']\n",
    "                labels = data['label']\n",
    "                tokenized_inputs = self.tokenizer(\n",
    "                    text,\n",
    "                    is_split_into_words=True,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                labels = self.align_labels(labels, tokenized_inputs.input_ids)\n",
    "                self.samples.append({\n",
    "                    'input_ids': tokenized_inputs.input_ids,\n",
    "                    'attention_mask': tokenized_inputs.attention_mask,\n",
    "                    'labels': labels\n",
    "                })\n",
    "\n",
    "    def _split_data(self):\n",
    "        split_index = int(len(self.samples) * self.split_ratio)\n",
    "        self.train_samples = self.samples[:split_index]\n",
    "        self.eval_samples = self.samples[split_index:]\n",
    "\n",
    "    import torch\n",
    "\n",
    "    def align_labels(self, labels, input_ids):\n",
    "        aligned_labels = []\n",
    "        label_idx = 0\n",
    "        for i in range(input_ids.size(1)):\n",
    "            token = self.tokenizer.convert_ids_to_tokens(input_ids[0, i].item())\n",
    "            # Check if the token is a special token (e.g., [CLS], [SEP], [PAD])\n",
    "            if token.startswith(\"##\") or token in [\"[CLS]\", \"[SEP]\"]:\n",
    "                aligned_labels.append(-100)  # Assign -100 to special tokens\n",
    "            else:\n",
    "                # Check if the current position corresponds to a label span\n",
    "                if label_idx < len(labels) and i >= labels[label_idx][0] and i <= labels[label_idx][1]:\n",
    "                    aligned_labels.append(self.label_map.get(labels[label_idx][2], -100))  # Assign label or -100 if not found\n",
    "                else:\n",
    "                    aligned_labels.append(-100)  # Assign -100 if no label is present for this position\n",
    "                # Move to the next label span if the end position is reached\n",
    "                if label_idx < len(labels) and i == labels[label_idx][1]:\n",
    "                    label_idx += 1\n",
    "        return torch.tensor([aligned_labels])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.train_samples[idx]\n",
    "        return sample\n",
    "\n",
    "    def get_eval_dataset(self):\n",
    "        return self.eval_samples\n",
    "\n",
    "class CustomDataCollatorForTokenClassification(DataCollatorForTokenClassification):\n",
    "    def __call__(self, features):\n",
    "        batch = {}\n",
    "        batch[\"input_ids\"] = torch.stack([feature[\"input_ids\"].squeeze() for feature in features])\n",
    "        batch[\"attention_mask\"] = torch.stack([feature[\"attention_mask\"].squeeze() for feature in features])\n",
    "        batch[\"labels\"] = torch.stack([feature[\"labels\"].squeeze() for feature in features])\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForTokenClassification.from_pretrained(\"legal_bert_ner_model5\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"legal_bert_ner_model5\")\n",
    "\n",
    "data_collator = CustomDataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"content/admin.jsonl\"\n",
    "\n",
    "# Define the label list\n",
    "label_list = [\"PRECEDENT\", \"LAWYER\", \"JUDGE\", \"RESPONDENT\", \"GPE\",\n",
    "              \"DATE\", \"OTHER_PERSON\", \"PROVISION\", \"ORG\", \"PETITIONER\",\n",
    "              \"WITNESS\", \"COURT\", \"STATUTE\", \"CASE_NUMBER\"]\n",
    "\n",
    "# Initialize dataset\n",
    "dataset2 = CustomDataset2(file_path, tokenizer, label_list)\n",
    "# Print a sample from the evaluation dataset\n",
    "sample = dataset2[0]\n",
    "print(\"Sample:\", sample)\n",
    "\n",
    "# Prepare training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "eval_dataset=dataset2.get_eval_dataset()\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset2,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the evaluation dataset\n",
    "eval_results = trainer.evaluate(eval_dataset=dataset2.get_eval_dataset())\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6daf033-a9f0-47e2-88c3-d5233a99bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (2, 512, 14)\n",
      "Sample predictions: [[[-0.76212335  1.3703938   0.6915548  ...  0.53673005 -0.6069215\n",
      "    0.81710404]\n",
      "  [ 0.15071033  0.7018597   0.95319915 ...  0.8818127  -0.78526247\n",
      "    1.0575448 ]\n",
      "  [-0.20709898  0.34900543  0.72603923 ...  0.66386414 -0.7663557\n",
      "    0.9665575 ]\n",
      "  ...\n",
      "  [-0.9306531   1.9120032   1.057309   ... -0.0152879  -0.38310352\n",
      "    0.35696003]\n",
      "  [-0.6381649   2.0791337   1.4263552  ...  0.03515275 -0.40768763\n",
      "    0.7192615 ]\n",
      "  [-0.52926654  0.7358375   0.2904504  ...  0.78621686 -0.18602158\n",
      "    1.3332641 ]]\n",
      "\n",
      " [[-0.5901942   1.033705    0.61574554 ...  0.75337225 -0.65282446\n",
      "    0.88701206]\n",
      "  [ 0.14502409  0.5168922   0.742612   ...  1.6869161  -0.8672279\n",
      "    0.5946497 ]\n",
      "  [-0.44689894  0.5852126   0.6448316  ...  1.4282548  -1.0369414\n",
      "    0.63747656]\n",
      "  ...\n",
      "  [-0.5909515   1.947429    1.2512305  ...  0.60361916 -0.6016633\n",
      "    0.99773526]\n",
      "  [-0.4940542   1.9479268   1.2256385  ...  0.517805   -0.70657283\n",
      "    0.5760161 ]\n",
      "  [-0.6150603   0.6197026   0.26472628 ...  1.0768335  -0.19810833\n",
      "    1.0625046 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the model\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "\n",
    "# Print the shape of the predictions\n",
    "print(\"Predictions shape:\", predictions.predictions.shape)\n",
    "\n",
    "# Print the first few predictions for inspection\n",
    "print(\"Sample predictions:\", predictions.predictions[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de2f8549-a1a8-468c-8392-6fae7b5025ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.098388671875\n",
      "Length of flat_labels: 4096\n",
      "Length of flat_predictions: 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/8mw4y10n4gq9sv5v2yj_xzjr0000gn/T/ipykernel_2734/3440932704.py:4: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  correct_predictions = np.sum(np.array_equal(true_label, pred_label) for true_label, pred_label in zip(flat_labels, flat_predictions))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate accuracy manually\n",
    "correct_predictions = np.sum(np.array_equal(true_label, pred_label) for true_label, pred_label in zip(flat_labels, flat_predictions))\n",
    "accuracy = correct_predictions / len(flat_predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "print(\"Length of flat_labels:\", len(flat_labels))\n",
    "print(\"Length of flat_predictions:\", len(flat_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b47c241c-bcd4-465e-9126-e7d1aee1add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ 3 11 11 ...  1  1 11]\n",
      "3\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Print the predictions\n",
    "print(\"Predictions:\", flat_predictions)\n",
    "for i in range(5):\n",
    "    print(flat_predictions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ad8194c-d6e4-49c0-8ec7-8f66c9db9c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de209798-49ca-4620-9d0c-e34b3199fb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
