{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a03422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"sectionized_data.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = simple_preprocess(text, deacc=True)  # Tokenize and remove accents\n",
    "    return tokens\n",
    "\n",
    "# Tokenize and preprocess the text\n",
    "df['tokens'] = df['Body'].apply(preprocess)\n",
    "\n",
    "#print(df['tokens'])\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = Dictionary(df['tokens'])\n",
    "\n",
    "# Bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac143b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jositav.2020\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (0) is identical to the `bos_token_id` (0), `eos_token_id` (None), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load pre-trained BERT legal model and tokenizer\n",
    "model_name = 'nlpaueb/legal-bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Filter out rows with empty token lists\n",
    "df = df[df['tokens'].map(len) > 0]\n",
    "\n",
    "# Encode your documents into BERT embeddings\n",
    "documents = df['tokens']  # Your list of preprocessed text documents\n",
    "\n",
    "# Encode each document\n",
    "encoded_documents = [tokenizer.encode(doc, add_special_tokens=True, max_length=512, truncation=True, padding='max_length', return_tensors='pt') for doc in documents]\n",
    "\n",
    "# Compute BERT embeddings for each document\n",
    "with torch.no_grad():\n",
    "    embeddings = []\n",
    "    for encoded_doc in encoded_documents:\n",
    "        outputs = model(encoded_doc)\n",
    "        pooled_output = outputs[1]  # Take the pooled output\n",
    "        embeddings.append(pooled_output.numpy())\n",
    "\n",
    "# Flatten the embeddings\n",
    "flat_embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# Initialize variables to store optimal values\n",
    "optimal_num_topics = 0\n",
    "max_coherence_score = -1\n",
    "\n",
    "# Range of possible numbers of topics to try\n",
    "num_topics_range = range(2, 100)\n",
    "\n",
    "# Iterate over different numbers of topics\n",
    "for num_topics in num_topics_range:\n",
    "    # Apply clustering (e.g., KMeans) to identify topics\n",
    "    kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "    topic_labels = kmeans.fit_predict(flat_embeddings)\n",
    "    \n",
    "    # Compute coherence score\n",
    "    coherence_model = CoherenceModel(topics=documents, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    \n",
    "    # Update optimal number of topics if coherence score is higher\n",
    "    if coherence_score > max_coherence_score:\n",
    "        optimal_num_topics = num_topics\n",
    "        max_coherence_score = coherence_score\n",
    "\n",
    "print(\"Optimal Number of Topics:\", optimal_num_topics)\n",
    "print(\"Max Coherence Score:\", max_coherence_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c01aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coeherance Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load pre-trained BERT Legal model and tokenizer\n",
    "model_name = 'nlpaueb/legal-bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Load English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Encode your documents into BERT embeddings\n",
    "documents = df['tokens']  # Your list of preprocessed text documents\n",
    "\n",
    "# Encode each document\n",
    "encoded_documents = [tokenizer.encode(doc, add_special_tokens=True, max_length=512, truncation=True, padding='max_length', return_tensors='pt') for doc in documents]\n",
    "\n",
    "# Compute BERT embeddings for each document\n",
    "with torch.no_grad():\n",
    "    embeddings = []\n",
    "    for encoded_doc in encoded_documents:\n",
    "        outputs = model(encoded_doc)\n",
    "        pooled_output = outputs[1]  # Take the pooled output\n",
    "        embeddings.append(pooled_output.numpy())\n",
    "\n",
    "# Flatten the embeddings\n",
    "flat_embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# Apply clustering (e.g., KMeans) to identify topics\n",
    "num_topics = 2\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "topic_labels = kmeans.fit_predict(flat_embeddings)\n",
    "\n",
    "# Print the assigned topic labels for each document\n",
    "document_topic_labels = {}\n",
    "for i, label in enumerate(topic_labels):\n",
    "    print(f\"Document {i}: Topic {label}\")\n",
    "    document_topic_labels[\"Document \" + str(i)] = \"Topic \" + str(label)\n",
    "    \n",
    "#print(document_topic_labels)\n",
    "# Collect documents assigned to each topic\n",
    "topic_documents = [[] for _ in range(num_topics)]\n",
    "for i, label in enumerate(topic_labels):\n",
    "    topic_documents[label].append(documents[i])\n",
    "\n",
    "# Tokenize the combined text of each topic and filter out stop words, punctuation, short words, and words containing '#'\n",
    "topic_tokenized_texts = []\n",
    "for docs in topic_documents:\n",
    "    # Ensure that each element in docs is a string\n",
    "    docs = [str(doc) for doc in docs]\n",
    "    combined_text = ' '.join(docs)\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(combined_text)\n",
    "    # Filter out stop words, punctuation, short words, and words containing '#'\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation and len(token) > 3 and '#' not in token]\n",
    "    topic_tokenized_texts.append(filtered_tokens)\n",
    "\n",
    "# Count the occurrences of each word in the tokenized text\n",
    "word_distributions = [Counter(tokens) for tokens in topic_tokenized_texts]\n",
    "\n",
    "# Calculate total number of words in each topic\n",
    "topic_word_counts = [sum(word_distribution.values()) for word_distribution in word_distributions]\n",
    "\n",
    "# Calculate the probability of each word in each topic and get the top 10 words\n",
    "top_words_probability = []\n",
    "for topic_idx, word_distribution in enumerate(word_distributions):\n",
    "    top_words = word_distribution.most_common(10)\n",
    "    topic_probability = {word: count / topic_word_counts[topic_idx] for word, count in top_words}\n",
    "    top_words_probability.append(topic_probability)\n",
    "\n",
    "# Create a dictionary to store the top words distribution for each topic\n",
    "top_words_distribution_dict = {}\n",
    "for topic_idx, topic_probability in enumerate(top_words_probability):\n",
    "    top_words_distribution_dict[f\"Topic {topic_idx + 1}\"] = topic_probability\n",
    "\n",
    "# Print the dictionary\n",
    "print(top_words_distribution_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f809e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot the word distribution for each topic\n",
    "def plot_topic_word_distribution(top_words_distribution_dict):\n",
    "    num_topics = len(top_words_distribution_dict)\n",
    "    fig, axes = plt.subplots(nrows=num_topics, ncols=1, figsize=(10, 6*num_topics))\n",
    "\n",
    "    for i, (topic, word_distribution) in enumerate(top_words_distribution_dict.items()):\n",
    "        words = list(word_distribution.keys())\n",
    "        probabilities = list(word_distribution.values())\n",
    "\n",
    "        ax = axes[i] if num_topics > 1 else axes\n",
    "        ax.barh(words, probabilities, color='skyblue')\n",
    "        ax.set_title(f'{topic}')\n",
    "        ax.set_xlabel('Probability')\n",
    "        ax.set_ylabel('Word')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the topic word distribution\n",
    "plot_topic_word_distribution(top_words_distribution_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79875f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dominant topic distribution\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "document_topic_distribution_dict = defaultdict(dict)\n",
    "\n",
    "# Count the occurrences of each topic label\n",
    "topic_counts = Counter(topic_labels)\n",
    "\n",
    "# Normalize the counts to get probabilities\n",
    "total_documents = len(documents)\n",
    "topic_distribution = {topic: count / total_documents for topic, count in topic_counts.items()}\n",
    "\n",
    "# Populate the dictionary with the topic distribution for each document\n",
    "for i, label in enumerate(topic_labels):\n",
    "    document_topic_distribution_dict[f\"Document {i+1}\"][f\"Topic {label}\"] = topic_distribution[label]\n",
    "\n",
    "# Print the dictionary\n",
    "for document, distribution in document_topic_distribution_dict.items():\n",
    "    print(document, \":\", distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40165428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the dictionary to a DataFrame for visualization\n",
    "df_distribution = pd.DataFrame(document_topic_distribution_dict).T.fillna(0)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_distribution, cmap=\"YlGnBu\", annot=True, fmt=\".3f\", cbar=False)\n",
    "plt.title(\"Probability Distribution of Topics Across Documents\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Document\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cface34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute pairwise cosine similarity between document embeddings\n",
    "def compute_coherence_score(embeddings, labels):\n",
    "    num_topics = len(set(labels))\n",
    "    coherence_scores = []\n",
    "    \n",
    "    for i in range(num_topics):\n",
    "        # Find documents assigned to the current topic\n",
    "        topic_documents = embeddings[labels == i]\n",
    "        \n",
    "        # Compute pairwise cosine similarity between documents in the same topic\n",
    "        similarity_matrix = cosine_similarity(topic_documents)\n",
    "        \n",
    "        # Compute the average pairwise similarity for the topic\n",
    "        average_similarity = similarity_matrix.mean()\n",
    "        \n",
    "        # Append the coherence score for the topic\n",
    "        coherence_scores.append(average_similarity)\n",
    "    \n",
    "    # Compute the overall coherence score as the mean of the coherence scores for all topics\n",
    "    overall_coherence_score = sum(coherence_scores) / num_topics\n",
    "    return overall_coherence_score\n",
    "\n",
    "# Compute coherence score\n",
    "coherence_score = compute_coherence_score(flat_embeddings, topic_labels)\n",
    "print(\"Coherence Score:\", coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping topic numbers to labels\n",
    "topics_word_distribution = top_words_distribution_dict\n",
    "\n",
    "topic_labels = {\n",
    "   \n",
    "    \n",
    "}\n",
    "\n",
    "# Replace topic numbers with labels\n",
    "labeled_document_topic = {}\n",
    "\n",
    "for docs, topic in document_topic_labels.items():\n",
    "    labeled_document_topic[docs] = topic_labels[topic]\n",
    "\n",
    "# Print the labeled topic distribution\n",
    "print(labeled_document_topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
